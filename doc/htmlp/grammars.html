<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" 
 "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html>
<head>
  <!-- $Id: grammars.html,v 1.34 2007/07/17 01:35:36 deraugla Exp $ -->
  <!-- Copyright (c) 2007 INRIA -->
  <title>extensible grammars</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <link rel="stylesheet" type="text/css" href="styles/base.css" />
</head>
<body>

<div id="menu">
</div>

<div id="content">

<h1 class="top">Extensible grammars</h1>

<p>This chapter describes the whole syntax and semantics of the
extensible grammars of camlp5.</p>

<p>The extensible grammars are the most advanced parsing tool of
camlp5. They apply to streams of characters using a lexer which has to
be previously defined by the programmer. In camlp5, the syntax of the
ocaml language is defined with extensible grammars, which makes camlp5
a bootstrapped system (it compiles its own features by itself).</p>

<div id="tableofcontents">
</div>

<h2>Getting started</h2>

<p>The extensible grammars are a system to build <em>grammar
entries</em> which can be extended dynamically. A grammar entry is an
abstract value internally containing a stream parser. The type of a
grammar entry is <tt>"Grammar.Entry.e t"</tt> where <tt>"t"</tt> is
the type of the values returned by the grammar entry.</p>

<p>To start with extensible grammars, it is necessary to build
a <em>grammar</em>, a value of type "<tt>Grammar.g</tt>", using
the function "<tt>Grammar.gcreate</tt>":</p>

<pre>
  value g = Grammar.gcreate lexer;
</pre>

<p>where "<tt>lexer</tt>" is a lexer previously defined. See the
section explaining the interface with lexers. In a first time, it is
possible to use a lexer of the module "<tt>Plexer</tt>" provided by
camlp5:</p>

<pre>
  value g = Grammar.gcreate (Plexer.gmake ());
</pre>

<p>Each grammar entry is associated with a grammar. Only grammar
entries of the same grammar can call each other. To create a grammar
entry, one has to use the function "<tt>Grammar.Entry.create</tt>" with
takes the grammar as first parameter and a name as second parameter. This
name is used in case of syntax errors. For example:</p>

<pre>
  value exp = Grammar.Entry.create g "expression";
</pre>

<p>To apply a grammar entry, the function
"<tt>Grammar.Entry.parse</tt>" can be used. Its first parameter is the
grammar entry, the second one a stream of characters:</p>

<pre>
  Grammar.Entry.parse exp (Stream.of_string "hello");
</pre>

<p>But if you experiment this, since the entry was just created
without any rules, you receive an error message:</p>

<pre>
  Stream.Error "entry [expression] is empty"
</pre>

<p>To add grammar rules to the grammar entry, it is necessary
to <em>extend</em> it, using a specific syntactic statement:
"<tt>EXTEND</tt>".</p>

<h2>Syntax of the EXTEND statement</h2>

<p>The "<tt>EXTEND</tt>" statement is added in the expressions of the
ocaml language when the syntax extension kit "<tt>pa_extend.cmo</tt>"
is loaded. Its syntax is:</p>

<pre>
    expression ::= extend
        extend ::= "EXTEND" extend-body "END"
   extend-body ::= global-opt entries
    global-opt ::= "GLOBAL" ":" entry-names ";"
                 | &lt;nothing&gt;
   entry-names ::= entry-name entry-names
                 | entry-name
         entry ::= entry-name ":" position-opt "[" levels "]"
  position-opt ::= "FIRST"
                 | "LAST"
                 | "BEFORE" label
                 | "AFTER" label
                 | "LEVEL" label
                 | &lt;nothing&gt;
        levels ::= level "|" levels
                 | level
         level ::= label-opt assoc-opt "[" rules "]"
     label-opt ::= label
                 | &lt;nothing&gt;
     assoc-opt ::= "LEFTA"
                 | "RIGHTA"
                 | "NONA"
                 | &lt;nothing&gt;
         rules ::= rule "|" rules
                 | rule
          rule ::= psymbols-opt "->" expression
                 | psymbols-opt
  psymbols-opt ::= psymbols
                 | &lt;nothing&gt;
      psymbols ::= psymbol ";" psymbols
                 | psymbol
       psymbol ::= symbol
                 | pattern "=" symbol
        symbol ::= keyword
                 | token
                 | token string
                 | entry-name
                 | entry-name "LEVEL" label
                 | "SELF"
                 | "NEXT"
                 | "LIST0" symbol
                 | "LIST0" symbol "SEP" symbol
                 | "LIST1" symbol
                 | "LIST1" symbol "SEP" symbol
                 | "OPT" symbol
                 | "[" rules "]"
                 | "(" symbol ")"
       keyword ::= string
         token ::= uident
         label ::= string
    entry-name ::= qualid
        qualid ::= qualid "." qualid
                 | uident
                 | lident
        uident ::= 'A'-'Z' ident
        lident ::= ('a'-'z' | '_' | utf8-char) ident
         ident ::= ident-char*
    ident-char ::= ('a'-'a' | 'A'-'Z' | '0'-'9' | '_' | ''' | utf8-char)
     utf8-char ::= '\128'-'\255'
</pre>

<p>Other statements, "<tt>GEXTEND</tt>", "<tt>DELETE_RULE</tt>",
"<tt>GDELETE_RULES</tt>" are also defined by the same syntax extension
kit. See further.</p>

<p>In the description above, ony "<tt>EXTEND</tt>" and "<tt>END</tt>"
are new keywords (reserved words which cannot be used in variables,
constructors or module names). The other strings
(e.g. "<tt>GLOBAL</tt>", "<tt>LEVEL</tt>", "<tt>LIST0</tt>",
"<tt>LEFTA</tt>", etc.) are not reserved.</p>

<h2>Semantics of the EXTEND statement</h2>

<p>The EXTEND statement starts with the "<tt>EXTEND</tt>" keyword and ends
with the "<tt>END</tt>" keyword.</p>

<h3>GLOBAL indicator</h3>

<p>After the first keyword, it is possible to see the identifier
"<tt>GLOBAL</tt>" followed by a colon, a list of entries names and a
semicolon. It says that these entries correspond to visible
(previously defined) entry variables, in the context of the EXTEND
statement, the other ones being locally and silently defined
inside.</p>

<ul>
  <li>If an entry, which is extended in the EXTEND statement, is in the
    GLOBAL list, but is not defined in the context of the EXTEND
    statement, the ocaml compiler will fail with the error "unbound
    value".</li>
  <li>If there is no GLOBAL indicator, and an entry, which is extended
    in the EXTEND statement, is not defined in the contex of the EXTEND
    statement, the ocaml compiler will also fail with the error "unbound
    value".</li>
</ul>

<p>Example:</p>

<pre>
  value exp = Grammar.Entry.create g "exp";
  EXTEND
    GLOBAL: exp;
    exp: [ [ x = foo; y = bar ] ];
    foo: [ [ "foo" ] ];
    bar: [ [ "bar" ] ];
  END;
</pre>

<p>The entry "exp" is an existing variable (defined by value exp =
...). On the other hand, the entries "foo" and "bar" have not been
defined. Because of the GLOBAL indicator, the system define them
locally.</p>

<p>Without the GLOBAL indicator, the three entries would have been
considered as global variables, therefore the ocaml compiler would
say "unbound variable" under the first undefined entry, "foo".</p>

<h3>Entries list</h3>

<p>Then the list of entries extensions follow. An entry extension
starts with the entry name followed by a colon. An entry may have
several levels corresponding to several stream parsers which call the
ones the others (see further).</p>

<h4>Optional position</h4>

<p>After the colon, it is possible to specify a where to insert the
defined levels:</p>

<ul>
  <li>The identifier "<tt>FIRST</tt>" (resp. "<tt>LAST</tt>")
    indicates that the level must be inserted before (resp. after) all
    possibly existing levels of the entry. They become their first
    (resp. last) levels.</li>
  <li>The identifier "<tt>BEFORE</tt>" (resp. "<tt>AFTER</tt>")
    followed by a level label (a string) indicates that the levels
    must be inserted before (resp. after) that level, if it exists. If
    it does not exist, the extend statement fails at run time.</li>
  <li>The identifier "<tt>LEVEL</tt>" followed by a level label
    indicates that the first level defined in the extend statement
    must be inserted at the given level, extending and modifying
    it. The other levels defined in the statement are inserted after
    this level, and before the possible levels following this
    level. If there is no level with this label, the extend statement
    fails at run time.</li>
  <li>By default, if the entry has no level, the levels defined in the
    statement are inserted in the entry. Otherwise the first defined
    level is inserted at the first level of the entry, extending or
    modifying it. The other levels are inserted afterwards (before the
    possible second level which may previously exist in the entry).</li>
</ul>

<h4>Levels</h4>

<p>After the optional "position", the <em>level</em> list follow. The
levels are separated by vertical bars, the whole list being between
brackets.</p>

<p>A level starts with an optional label, which corresponds to its name.
This label is useful to specify this level in case of future extension,
using the <em>position</em> (see previous section).</p>

<p>The level continues with an optional associativity indicator, which
can be:</p>

<ul>
  <li>LEFTA for left associativity (default),</li>
  <li>RIGHTA for right associativity,</li>
  <li>NONA for no associativity.</li>
</ul>

<h4>Rules</h4>

<p>At last, the grammar <em>rule</em> list appear. The rules are
separated by vertical bars, the whole list being brackets.</p>

<p>A rule looks like a match case in the "<tt>match</tt>" statement or
a parser case in the "<tt>parser</tt>" statement: a list of psymbols
(see next paragraph) separated by semicolons, followed by a right
arrow and an expression, the semantic action. Actually, the right
arrow and expression are optional: in this case, it is equivalent to
an expression which would be the unit "<tt>()</tt>" constructor.</p>

<p>A psymbol is either a pattern, followed with the equal sign and a
symbol, or by a symbol alone. It corresponds to a test of this symbol,
whose value is bound to the pattern if any.</p>

<h4>Symbols</h4>

<p>A symbol is either:</p>

<ul>
  <li>a keyword (a string): the input must match this keyword,</li>
  <li>a token name (an identifier starting with an uppercase
    character), optionally followed by a string: the input must match
    this token (any value if no string, or that string if a string
    follows the token name), the list of the available tokens
    depending on the associated lexer (the list of tokens available
    with "Plexer.gmake ()" is: LIDENT, UIDENT, TILDEIDENT,
    TILDEIDENTCOLON, QUESTIONIDENT, INT, INT_l, INT_L, INT_n, FLOAT,
    CHAR, STRING, QUOTATION, ANTIQUOT and EOI; other lexers may
    propose other lists of tokens),
</li>
  <li>an entry name, which correspond to a call to this entry,</li>
  <li>an entry name followed by the identifier "<tt>LEVEL</tt>" and a
    level label, which correspond to the call to this entry at that
    level,</li>
  <li>the identifier "<tt>SELF</tt>" which is a recursive call to the
    present entry, according to the associativity (i.e. it may be a
    call at the current level, to the next level, or to the top level
    of the entry): "<tt>SELF</tt>" is equivalent to the name of the
    entry itself,</li>
  <li>the identifier "<tt>NEXT</tt>", which is a call to the next level
    of the current entry,</li>
  <li>a left brace, followed by a list of rules separated by vertical
    bars, and a right brace: equivalent to a call to an entry, with
    these rules, inlined,</li>
  <li>a meta symbol (see further),</li>
  <li>a symbol between parentheses.</li>
</ul>

<p>The syntactic analysis follow the list of symbols. If it fails,
depending on the first items of the rule (see the section about the
kind of grammars recognized):</p>

<ul>
  <li>the parsing may fail by raising the exception
    "<tt>Stream.Error</tt>"</li>
  <li>the parsing may continue with the next rule.</li>
</ul>

<h4>Meta symbols</h4>

<p>Extra symbols exist, allowing to manipulate lists or optional
symbols. They are:</p>

<ul>
  <li>LIST0 followed by a symbol: this is a list of this symbol,
    possibly empty,</li>
  <li>LIST0 followed by a symbol, SEP and another symbol: this is a
    list, possibly empty, of the first symbol separated by the second
    one,</li>
  <li>LIST1 followed by a symbol: this is a list of this symbol,
    with at least one element,</li>
  <li>LIST0 followed by a symbol, SEP and another symbol: this is a
    list, with at least one element, of the first symbol separated by
    the second one,</li>
  <li>OPT followed by a symbol: equivalent to "this symbol or
    nothing".</li>
</ul>

<h3>Rules insertion</h3>

<p>Remember that "<tt>EXTEND</tt>" is a statement, not a declaration:
the rules are added in the entries at run time. Each rule is
internally inserted in a tree, allowing the left factorization of the
rule. For example, with this list of rules (borrowed from the camlp5
sources):</p>

<pre>
  "method"; "private"; "virtual"; l = label; ":"; t = poly_type
  "method"; "virtual"; "private"; l = label; ":"; t = poly_type
  "method"; "virtual"; l = label; ":"; t = poly_type
  "method"; "private"; l = label; ":"; t = poly_type; "="; e = expr
  "method"; "private"; l = label; sb = fun_binding
  "method"; l = label; ":"; t = poly_type; "="; e = expr
  "method"; l = label; sb = fun_binding
</pre>

<p>the rules are inserted in a tree and the result looks like:</p>

<pre>
  "method"
     |-- "private"
     |       |-- "virtual"
     |       |       |-- label
     |       |             |-- ":"
     |       |                  |-- poly_type
     |       |-- label
     |             |-- ":"
     |             |    |-- poly_type
     |             |            |-- ":="
     |             |            |    |-- expr
     |             |-- fun_binding
     |-- "virtual"
     |       |-- "private"
     |       |       |-- label
     |       |             |-- ":"
     |       |                  |-- poly_type
     |       |-- label
     |             |-- ":"
     |                  |-- poly_type
     |-- label
           |-- ":"
           |    |-- poly_type
           |            |-- "="
           |                 |-- expr
           |-- fun_binding
</pre>

<p>This tree is built as long as rules are inserted. When used, by
applying the function "<tt>Grammar.Entry.parse</tt>" to the current
entry, the input is matched with that tree, starting from the tree
root, descending on it as long as the parsing advances.</p>

<p>There is a different tree by entry level.</p>

<h3>Semantic action</h3>

<p>The semantic action, i.e. the expression following the right arrow
in rules, contain in its environment:</p>

<ul>
  <li>the variables bound by the patterns of the symbols found in the
    rules,</li>
  <li>the specific variable "<tt>loc</tt>" which contain the location
    of the whole rule in the source.</li>
</ul>

<p>The location is an abstract type defined in the module
"<tt>Stdpp</tt>" of camlp5.</p>

<p>It is possible to change the name of this variable by using the option
"<tt>-loc</tt>" of camlp5. For example, compiling a file like this:</p>

<pre>
  camlp5r -loc foobar file.ml
</pre>

<p>the variable name, for the location will be "<tt>foobar</tt>"
instead of "<tt>loc</tt>".</p>

<h2>Syntax of the DELETE_RULE statement</h2>

<h2>Semantics of the DELETE_RULE statement</h2>

<h2>Machinery</h2>

<p>Each entry level contains internally two trees:</p>

<ul>
  <li>A tree of the rules starting with the current entry name or by
    the identifier "<tt>SELF</tt>",</li>
  <li>A tree of the rules <em>not</em> starting with the current entry
    name nor by "<tt>SELF</tt>".</li>
</ul>

<p>... to be completed ...</p>

<h2>Kind of grammar</h2>

<p>... to be completed ...</p>

<h2>Interface with the lexer</h2>

<p>To create a grammar, the function "<tt>Grammar.gcreate</tt>" must be
called, with a lexer as parameter.</p>

<p>A simple solution, as possible lexer, is the predefined lexer built
by "<tt>Plexer.gmake ()</tt>", lexer used for the ocaml grammar of
camlp5. In this case, you can just put it as parameter of
"<tt>Grammar.gcreate</tt>" and it is not necessary to read this
section.</p>

<p>Otherwise, it is necessary to create a value of type
"<tt>Token.glexer t</tt>", where "<tt>t</tt>" is the token type, to
give as parameter to "<tt>Grammar.create</tt>". This type is defined
like this:</p>

<pre>
  type glexer 't =
    { tok_func : lexer_func 't;
      tok_match : pattern -> 't -> string;
      tok_using : pattern -> unit;
      tok_removing : pattern -> unit;
      tok_text : pattern -> string;
      tok_comm : mutable option (list location) }
  ;
</pre>

<p>The types used in this type are defined as long as the explanations
of the next sections.</p>

<h3>The token type</h3>

<p>The token type, which is the type parameter of the
"<tt>glexer</tt>" type, can any user type.</p>

<p>Values of the token type will be returned by the lexer, and the way
the values of this type will be matched against the rules symbols will
be defined in the "match" function.</p>

<h3>The lexer</h3>

<p>The lexer is the field "<tt>tok_func</tt>" defined above. Its type
is defined like this:</p>

<pre>
  type lexer_func 't = Stream.t char -> (Stream.t 't * location_function);
</pre>

<p>The location function is a function taking the token count
  (starting from zero) as parameter and returning its location in the
  source. It is important if you want to get the good locations in the
  semantic actions of the grammar rules.</p>

<p>Notice that, despite the lexer takes a character stream as
  parameter, you are not obliged to use the stream parsers technology
  to write your lexer. What is important is that it does the job.</p>

<h3>Token patterns</h3>

<p>Before going further, it is important to understand the notion
  of <em>token patterns</em>. A token pattern is a value of the type
  defined like this:</p>

<pre>
  type pattern = (string * string);
</pre>

<p>This type represents values of the token symbols and keywords in
  the grammar rules.</p>

<p>As one can see, it is a couple of strings:</p>

<ul>
  <li>The first string is the <em>constructor name</em>, an identifier
    starting with an uppercase character:
    <ul>
      <li>for a token symbol, it is its name,</li>
      <li>for a keyword, it is the empty string.</li>
    </ul>
  </li>

  <li>The second string is the <em>parameter</em>:
    <ul>
      <li>for a token symbol:
        <ul>
          <li>if the string is empty, it indicates any value (like the
            pattern "<tt>_</tt>" in the ocaml language)</li>
          <li>if the string is not empty, it indicates a match of this
            constructor with that value.</li>
        </ul>
      <li>for a keyword, it is the keyword itself.</li>
    </ul>
  </li>
</ul>

<p>For example, in the following grammar rule:</p>

<pre>
  "for"; i = LIDENT; "="; e1 = SELF; "to"; e2 = SELF
</pre>

<p>the different symbols and keywords are represented by the following
token patterns:</p>

<ul>
  <li>the keyword "for" is represented by <tt>("", "for")</tt>,</li>
  <li>the keyword "=" by <tt>("", "=")</tt>,</li>
  <li>the keyword "to" by <tt>("", "to")</tt>),</li>
  <li>and the token symbol <tt>LIDENT</tt> by <tt>("LIDENT", "")</tt>.</li>
</ul>

<p>The symbol <tt>UIDENT "Foo"</tt> in a rule would be represented
by the token pattern:</p>

<pre>
  ("UIDENT", "Foo")
</pre>

<p>Notice that the symbol "<tt>SELF</tt>" is a specific symbol of the
EXTEND syntax: it does not correspond to a token pattern and is
represented differently. A token constructor name must not belong to
the specific symbols: SELF, NEXT, LIST0, LIST1 and OPT.</p>

<h3>Match function</h3>

<p>The "match" function, the field "<tt>tok_match</tt>", indicates how
  a token is matched against a token symbol of a grammar rule, i.e. a
  token pattern.</p>

<p>If you choose the token type identical to token pattern type, i.e.
  "<tt>(string * string)</tt>", a simple and direct solution is to use
  the function "<tt>default_match</tt>" defined in the module
  "<tt>Token</tt>" which does the job. In this case, you can ignore
  the rest of this section.</p>

<p>Otherwise, if you want to use your own specific token type (or if
  you don't have the choice), you have to write your specific match
  function.</p>

<p>This function takes a token pattern as parameter and return a
  function matching a token, returning the matched string or raising
  the exception "<tt>Stream.Failure</tt>" if the token does not
  match.</p>

<p>Example. If the token type is defined like this:</p>

<pre>
  type mytoken = [ Ident of string | Int of int | Comma | Equal | Keyw of string ];
</pre>

<p>a possible implementation of the "match" function could be that:</p>

<pre>
  value mymatch =
    fun
    [ ("IDENT", "") -> fun [ Ident s -> s | _ -> raise Stream.Failure ]
    | ("INT", "") -> fun [ Int i -> string_of_int i | _ -> raise Stream.Failure ]
    | ("", ",") -> fun [ Comma -> "" | _ -> raise Stream.Failure ]
    | ("", "=") -> fun [ Equal -> "" | _ -> raise Stream.Failure ]
    | ("", s) ->
        fun
        [ Keyw k -> if k = s then "" else raise Stream.Failure
        | _ -> raise Stream.Failure ] ]
  ;
</pre>

<p>Notice that, for efficiency, it is necessary to write this function
  like above, as a match of token patterns returning, for each case, the
  function which matches the token, <em>not</em> a function matching the
  token pattern and the token together and returning a string for each
  case.</p>

<p>Another example is the "default_match" function proposed above for
  users who don't want to busy about that and choosed the token type
  identical to the token pattern type. The code of this function is
  something like:</p>

<pre>
  value default_match =
    fun
    [ (p_con, "") ->
        fun (con, prm) -> if con = p_con then prm else raise Stream.Failure
    | (p_con, p_prm) ->
        fun (con, prm) ->
          if con = p_con && prm = p_prm then prm else raise Stream.Failure ]
  ;
</pre>

<p>... to be completed ...</p>

<h3>...</h3>

<p>Two extra functions, of type "<tt>Token.pattern -> unit</tt>" have
  to be written: the <em>using</em> and <em>removing</em>
  functions.</p>

<p>If you don't want to busy about them, is is acceptable, as a first
  version, to write functions doing nothing, just returning the unit
  value "<tt>()</tt>". If you choose this option, you can skip the
  rest of this section.</p>

<p>But there is then a little risk: if the EXTEND statement contain
  rules with mispelled token symbols, the system will not detect them,
  and the grammar may not work.</p>

<p>For example, if the following rule (example coming from camlp5
sources):</p>

<pre>
  "external"; i = LIDENT; ":"; t = ctyp; "="; sl = LIST1 STRING
</pre>

<p>is written, by error, like this:</p>

<pre>
  "external"; i = LODENT; ":"; t = ctyp; "="; sl = LIST1 STRANG
</pre>

<p>and if the "using" function does nothing, there is no check that
  LIDENT has been mispelled as LODENT, nor that STRING have been
  mispelled as STRANG. In this case, the rule will never work, but
  nothing would have told you.</p>

<p>The check is done while running the EXTEND statement. The system
  scans all rules, and calls the "using" function with all token and
  keywords found in all rules.</p>

<p>This "using" function can then check the correctness of the token
  patterns with a simple pattern matching.</p>

<p>Morevover, the keywords (whose syntax is just a string in the
  EXTEND statement) may be accepted or not by the lexer. It is also
  possible that the lexer accepts to add new keywords, as long as they
  are found in the rules of the EXTEND statements.</p>

<p>The "using" function takes a token pattern as parameter and must
answer:</p>

<ul>
  <li>either nothing (the unit value "<tt>()</tt>") to say: this token
    is ok, and, in case of keyword, may decide to add, as side effect,
    this keyword in its tables,</li>
  <li>or raise the exception "<tt>Token.Error</tt>" with an error
    message to make the EXTEND statement fail.</li>
</ul>

<p>This problem of risk of mispelling tokens could also be checked by
  the "match" function (see the section about this function).</p>

<p>The "removing" function is called in case of the DELETE_RULE
  statement, for all token patterns found in the rule. It allows, for
  example, to possibly remove keywords (if a count of keywords usage
  is maintained by the lexer).</p>

<h3>...</h3>

<p>The function "<tt>Grammar.gcreate</tt>" takes a g-lexer as
parameter. A g-lexer is a value of type "<tt>Token.glexer t</tt>"
where "<tt>t</tt>" is the type of the tokens. The programmer of the
lexer may choose any type he wants, on condition that he provides also
a correct function "<tt>tok_match</tt>" (see further) which tells how
a token pattern is matched against this token type.</p>

<p>A suitable value for a g-lexer is the result of "<tt>Plexer.gmake
()</tt>" which is the lexer used for the ocaml language in camlp5.</p>

<h3>...</h3>

<p>The type "<tt>Token.glexer</tt>" is a record type containing six
fields:</p>

<ul>
  <li>tok_func: function returning a token stream from a char stream,</li>
  <li>tok_using: function checking if a token pattern is recognized by
  the lexer,</li>
  <li>tok_removing: function telling the lexer that a token pattern
  has been removed,</li>
  <li>tok_match: function telling how a token pattern is parsed,</li>
  <li>tok_text: function returning the name of a token pattern,</li>
  <li>tok_comm: option value to ask the lexer to record the locations
  of the comments.</li>
</ul>

<p>The function "<tt>tok_using</tt>" is called by the EXTEND statement
while treating the grammar rules. When encountering a keyword or a token
symbol, this function is called:</p>

<ul>
  <li>if it is a token pattern, to check that this token is really
    recognized by the lexer,</li>
  <li>if it is a keyword, to check that also, or to allow the lexer
    to record this keyword in its tables.</li>
</ul>

<p>Since this list is a little bit complicated, there are default
functions and values that can be used in the first time:</p>

<ul>
  <li>Token.lexer_func_of_parser is a provided function returning a
    value suitable value for the field tok_func from a char stream
    parser,</li>
  <li>Token.default_match is an acceptable function, suitable for the
    field tok_match,</li>
  <li>Token.lexer_text is an acceptable function, suitable for the
    field tok_text,</li>
  <li>the function "<tt>fun _ -> ()</tt>" can be used for the fields
    tok_using and tok_removing in a first time,</li>
  <li>the field tok_comm can be set to None.</li>
</ul>

<p>Example: if you have a lexer function named 'lexer', you can make the
glexer value like this:</p>

<pre>
  {Token.tok_func = lexer;
   Token.tok_using _ = (); Token.tok_removing _ = ();
   Token.tok_match = Token.default_match;
   Token.tok_text = Token.lexer_text;
   Token.tok_comm = None}
</pre>

<p>... to be completed ...</p>

<h2>Functorial interface</h2>

<p>... to be completed ...</p>

<div class="trailer">
</div>

</div>

</body>
</html>
