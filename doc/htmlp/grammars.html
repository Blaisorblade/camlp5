<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" 
 "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html>
<head>
  <!-- $Id: grammars.html,v 1.57 2007/07/19 08:44:14 deraugla Exp $ -->
  <!-- Copyright (c) 2007 INRIA -->
  <title>extensible grammars</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <link rel="stylesheet" type="text/css" href="styles/base.css" />
</head>
<body>

<div id="menu">
</div>

<div id="content">

<h1 class="top">Extensible grammars</h1>

<p>This chapter describes the whole syntax and semantics of the
extensible grammars of camlp5.</p>

<p>The extensible grammars are the most advanced parsing tool of
camlp5. They apply to streams of characters using a lexer which has to
be previously defined by the programmer. In camlp5, the syntax of the
ocaml language is defined with extensible grammars, which makes camlp5
a bootstrapped system (it compiles its own features by itself).</p>

<div id="tableofcontents">
</div>

<h2>Getting started</h2>

<p>The extensible grammars are a system to build <em>grammar
entries</em> which can be extended dynamically. A grammar entry is an
abstract value internally containing a stream parser. The type of a
grammar entry is <tt>"Grammar.Entry.e t"</tt> where <tt>"t"</tt> is
the type of the values returned by the grammar entry.</p>

<p>To start with extensible grammars, it is necessary to build
a <em>grammar</em>, a value of type "<tt>Grammar.g</tt>", using
the function "<tt>Grammar.gcreate</tt>":</p>

<pre>
  value g = Grammar.gcreate lexer;
</pre>

<p>where "<tt>lexer</tt>" is a lexer previously defined. See the
section explaining the interface with lexers. In a first time, it is
possible to use a lexer of the module "<tt>Plexer</tt>" provided by
camlp5:</p>

<pre>
  value g = Grammar.gcreate (Plexer.gmake ());
</pre>

<p>Each grammar entry is associated with a grammar. Only grammar
entries of the same grammar can call each other. To create a grammar
entry, one has to use the function "<tt>Grammar.Entry.create</tt>" with
takes the grammar as first parameter and a name as second parameter. This
name is used in case of syntax errors. For example:</p>

<pre>
  value exp = Grammar.Entry.create g "expression";
</pre>

<p>To apply a grammar entry, the function
"<tt>Grammar.Entry.parse</tt>" can be used. Its first parameter is the
grammar entry, the second one a stream of characters:</p>

<pre>
  Grammar.Entry.parse exp (Stream.of_string "hello");
</pre>

<p>But if you experiment this, since the entry was just created
without any rules, you receive an error message:</p>

<pre>
  Stream.Error "entry [expression] is empty"
</pre>

<p>To add grammar rules to the grammar entry, it is necessary
to <em>extend</em> it, using a specific syntactic statement:
"<tt>EXTEND</tt>".</p>

<h2>Syntax of the EXTEND statement</h2>

<p>The "<tt>EXTEND</tt>" statement is added in the expressions of the
  ocaml language when the syntax extension kit "<tt>pa_extend.cmo</tt>"
  is loaded. Its syntax is:</p>

<pre>
    expression ::= extend
        extend ::= "EXTEND" extend-body "END"
   extend-body ::= global-opt entries
    global-opt ::= "GLOBAL" ":" entry-names ";"
                 | &lt;nothing&gt;
   entry-names ::= entry-name entry-names
                 | entry-name
         entry ::= entry-name ":" position-opt "[" levels "]"
  position-opt ::= "FIRST"
                 | "LAST"
                 | "BEFORE" label
                 | "AFTER" label
                 | "LEVEL" label
                 | &lt;nothing&gt;
        levels ::= level "|" levels
                 | level
         level ::= label-opt assoc-opt "[" rules "]"
     label-opt ::= label
                 | &lt;nothing&gt;
     assoc-opt ::= "LEFTA"
                 | "RIGHTA"
                 | "NONA"
                 | &lt;nothing&gt;
         rules ::= rule "|" rules
                 | rule
          rule ::= psymbols-opt "->" expression
                 | psymbols-opt
  psymbols-opt ::= psymbols
                 | &lt;nothing&gt;
      psymbols ::= psymbol ";" psymbols
                 | psymbol
       psymbol ::= symbol
                 | pattern "=" symbol
        symbol ::= keyword
                 | token
                 | token string
                 | entry-name
                 | entry-name "LEVEL" label
                 | "SELF"
                 | "NEXT"
                 | "LIST0" symbol
                 | "LIST0" symbol "SEP" symbol
                 | "LIST1" symbol
                 | "LIST1" symbol "SEP" symbol
                 | "OPT" symbol
                 | "[" rules "]"
                 | "(" symbol ")"
       keyword ::= string
         token ::= uident
         label ::= string
    entry-name ::= qualid
        qualid ::= qualid "." qualid
                 | uident
                 | lident
        uident ::= 'A'-'Z' ident
        lident ::= ('a'-'z' | '_' | utf8-char) ident
         ident ::= ident-char*
    ident-char ::= ('a'-'a' | 'A'-'Z' | '0'-'9' | '_' | ''' | utf8-char)
     utf8-char ::= '\128'-'\255'
</pre>

<p>Other statements, "<tt>GEXTEND</tt>", "<tt>DELETE_RULE</tt>",
  "<tt>GDELETE_RULE</tt>" are also defined by the same syntax extension
  kit. See further.</p>

<p>In the description above, ony "<tt>EXTEND</tt>" and "<tt>END</tt>"
  are new keywords (reserved words which cannot be used in variables,
  constructors or module names). The other strings
  (e.g. "<tt>GLOBAL</tt>", "<tt>LEVEL</tt>", "<tt>LIST0</tt>",
  "<tt>LEFTA</tt>", etc.) are not reserved.</p>

<h2>Semantics of the EXTEND statement</h2>

<p>The EXTEND statement starts with the "<tt>EXTEND</tt>" keyword and ends
  with the "<tt>END</tt>" keyword.</p>

<h3>GLOBAL indicator</h3>

<p>After the first keyword, it is possible to see the identifier
"<tt>GLOBAL</tt>" followed by a colon, a list of entries names and a
semicolon. It says that these entries correspond to visible
(previously defined) entry variables, in the context of the EXTEND
statement, the other ones being locally and silently defined
inside.</p>

<ul>
  <li>If an entry, which is extended in the EXTEND statement, is in the
    GLOBAL list, but is not defined in the context of the EXTEND
    statement, the ocaml compiler will fail with the error "unbound
    value".</li>
  <li>If there is no GLOBAL indicator, and an entry, which is extended
    in the EXTEND statement, is not defined in the contex of the EXTEND
    statement, the ocaml compiler will also fail with the error "unbound
    value".</li>
</ul>

<p>Example:</p>

<pre>
  value exp = Grammar.Entry.create g "exp";
  EXTEND
    GLOBAL: exp;
    exp: [ [ x = foo; y = bar ] ];
    foo: [ [ "foo" ] ];
    bar: [ [ "bar" ] ];
  END;
</pre>

<p>The entry "exp" is an existing variable (defined by value exp =
...). On the other hand, the entries "foo" and "bar" have not been
defined. Because of the GLOBAL indicator, the system define them
locally.</p>

<p>Without the GLOBAL indicator, the three entries would have been
considered as global variables, therefore the ocaml compiler would
say "unbound variable" under the first undefined entry, "foo".</p>

<h3>Entries list</h3>

<p>Then the list of entries extensions follow. An entry extension
starts with the entry name followed by a colon. An entry may have
several levels corresponding to several stream parsers which call the
ones the others (see further).</p>

<h4>Optional position</h4>

<p>After the colon, it is possible to specify a where to insert the
defined levels:</p>

<ul>
  <li>The identifier "<tt>FIRST</tt>" (resp. "<tt>LAST</tt>")
    indicates that the level must be inserted before (resp. after) all
    possibly existing levels of the entry. They become their first
    (resp. last) levels.</li>
  <li>The identifier "<tt>BEFORE</tt>" (resp. "<tt>AFTER</tt>")
    followed by a level label (a string) indicates that the levels
    must be inserted before (resp. after) that level, if it exists. If
    it does not exist, the extend statement fails at run time.</li>
  <li>The identifier "<tt>LEVEL</tt>" followed by a level label
    indicates that the first level defined in the extend statement
    must be inserted at the given level, extending and modifying
    it. The other levels defined in the statement are inserted after
    this level, and before the possible levels following this
    level. If there is no level with this label, the extend statement
    fails at run time.</li>
  <li>By default, if the entry has no level, the levels defined in the
    statement are inserted in the entry. Otherwise the first defined
    level is inserted at the first level of the entry, extending or
    modifying it. The other levels are inserted afterwards (before the
    possible second level which may previously exist in the entry).</li>
</ul>

<h4>Levels</h4>

<p>After the optional "position", the <em>level</em> list follow. The
levels are separated by vertical bars, the whole list being between
brackets.</p>

<p>A level starts with an optional label, which corresponds to its name.
This label is useful to specify this level in case of future extension,
using the <em>position</em> (see previous section).</p>

<p>The level continues with an optional associativity indicator, which
can be:</p>

<ul>
  <li>LEFTA for left associativity (default),</li>
  <li>RIGHTA for right associativity,</li>
  <li>NONA for no associativity.</li>
</ul>

<h4>Rules</h4>

<p>At last, the grammar <em>rule</em> list appear. The rules are
separated by vertical bars, the whole list being brackets.</p>

<p>A rule looks like a match case in the "<tt>match</tt>" statement or
a parser case in the "<tt>parser</tt>" statement: a list of psymbols
(see next paragraph) separated by semicolons, followed by a right
arrow and an expression, the semantic action. Actually, the right
arrow and expression are optional: in this case, it is equivalent to
an expression which would be the unit "<tt>()</tt>" constructor.</p>

<p>A psymbol is either a pattern, followed with the equal sign and a
symbol, or by a symbol alone. It corresponds to a test of this symbol,
whose value is bound to the pattern if any.</p>

<h4>Symbols</h4>

<p>A symbol is either:</p>

<ul>
  <li>a keyword (a string): the input must match this keyword,</li>
  <li>a token name (an identifier starting with an uppercase
    character), optionally followed by a string: the input must match
    this token (any value if no string, or that string if a string
    follows the token name), the list of the available tokens
    depending on the associated lexer (the list of tokens available
    with "Plexer.gmake ()" is: LIDENT, UIDENT, TILDEIDENT,
    TILDEIDENTCOLON, QUESTIONIDENT, INT, INT_l, INT_L, INT_n, FLOAT,
    CHAR, STRING, QUOTATION, ANTIQUOT and EOI; other lexers may
    propose other lists of tokens),
</li>
  <li>an entry name, which correspond to a call to this entry,</li>
  <li>an entry name followed by the identifier "<tt>LEVEL</tt>" and a
    level label, which correspond to the call to this entry at that
    level,</li>
  <li>the identifier "<tt>SELF</tt>" which is a recursive call to the
    present entry, according to the associativity (i.e. it may be a
    call at the current level, to the next level, or to the top level
    of the entry): "<tt>SELF</tt>" is equivalent to the name of the
    entry itself,</li>
  <li>the identifier "<tt>NEXT</tt>", which is a call to the next level
    of the current entry,</li>
  <li>a left brace, followed by a list of rules separated by vertical
    bars, and a right brace: equivalent to a call to an entry, with
    these rules, inlined,</li>
  <li>a meta symbol (see further),</li>
  <li>a symbol between parentheses.</li>
</ul>

<p>The syntactic analysis follow the list of symbols. If it fails,
depending on the first items of the rule (see the section about the
kind of grammars recognized):</p>

<ul>
  <li>the parsing may fail by raising the exception
    "<tt>Stream.Error</tt>"</li>
  <li>the parsing may continue with the next rule.</li>
</ul>

<h4>Meta symbols</h4>

<p>Extra symbols exist, allowing to manipulate lists or optional
symbols. They are:</p>

<ul>
  <li>LIST0 followed by a symbol: this is a list of this symbol,
    possibly empty,</li>
  <li>LIST0 followed by a symbol, SEP and another symbol: this is a
    list, possibly empty, of the first symbol separated by the second
    one,</li>
  <li>LIST1 followed by a symbol: this is a list of this symbol,
    with at least one element,</li>
  <li>LIST0 followed by a symbol, SEP and another symbol: this is a
    list, with at least one element, of the first symbol separated by
    the second one,</li>
  <li>OPT followed by a symbol: equivalent to "this symbol or
    nothing".</li>
</ul>

<h3>Rules insertion</h3>

<p>Remember that "<tt>EXTEND</tt>" is a statement, not a declaration:
the rules are added in the entries at run time. Each rule is
internally inserted in a tree, allowing the left factorization of the
rule. For example, with this list of rules (borrowed from the camlp5
sources):</p>

<pre>
  "method"; "private"; "virtual"; l = label; ":"; t = poly_type
  "method"; "virtual"; "private"; l = label; ":"; t = poly_type
  "method"; "virtual"; l = label; ":"; t = poly_type
  "method"; "private"; l = label; ":"; t = poly_type; "="; e = expr
  "method"; "private"; l = label; sb = fun_binding
  "method"; l = label; ":"; t = poly_type; "="; e = expr
  "method"; l = label; sb = fun_binding
</pre>

<p>the rules are inserted in a tree and the result looks like:</p>

<pre>
  "method"
     |-- "private"
     |       |-- "virtual"
     |       |       |-- label
     |       |             |-- ":"
     |       |                  |-- poly_type
     |       |-- label
     |             |-- ":"
     |             |    |-- poly_type
     |             |            |-- ":="
     |             |                 |-- expr
     |             |-- fun_binding
     |-- "virtual"
     |       |-- "private"
     |       |       |-- label
     |       |             |-- ":"
     |       |                  |-- poly_type
     |       |-- label
     |             |-- ":"
     |                  |-- poly_type
     |-- label
           |-- ":"
           |    |-- poly_type
           |            |-- "="
           |                 |-- expr
           |-- fun_binding
</pre>

<p>This tree is built as long as rules are inserted. When used, by
applying the function "<tt>Grammar.Entry.parse</tt>" to the current
entry, the input is matched with that tree, starting from the tree
root, descending on it as long as the parsing advances.</p>

<p>There is a different tree by entry level.</p>

<h3>Semantic action</h3>

<p>The semantic action, i.e. the expression following the right arrow
in rules, contain in its environment:</p>

<ul>
  <li>the variables bound by the patterns of the symbols found in the
    rules,</li>
  <li>the specific variable "<tt>loc</tt>" which contain the location
    of the whole rule in the source.</li>
</ul>

<p>The location is an abstract type defined in the module
"<tt>Stdpp</tt>" of camlp5.</p>

<p>It is possible to change the name of this variable by using the option
"<tt>-loc</tt>" of camlp5. For example, compiling a file like this:</p>

<pre>
  camlp5r -loc foobar file.ml
</pre>

<p>the variable name, for the location will be "<tt>foobar</tt>"
instead of "<tt>loc</tt>".</p>

<h2>The DELETE_RULE statement</h2>

<p>The "<tt>DELETE_RULE</tt>" statement is also added in the
  expressions of the ocaml language when the syntax extension kit
  "<tt>pa_extend.cmo</tt>" is loaded. Its syntax is:</p>

<pre>
        expression ::= delete-rule
       delete-rule ::= "DELETE_RULE" delete-rule-body "END"
  delete-rule-body ::= entry-name ":" symbols
           symbols ::= symbol symbols
                     | symbol
</pre>

<p>See the syntax of the EXTEND statement for the meaning of the syntax
  entries not defined above.</p>

<p>The entry is scanned for a rule matching the giving symbol
  list. When found, the rule is removed. If no rule is found, the
  exception "<tt>Not_found</tt>" is raised.</p>

<h2>Grammar machinery</h2>

<p>We explain here the detail of the mechanism of the parsing of an
  entry.</p>

<h3>Start and Continue</h3>

<p>At each entry level, the rules are separated into two trees:</p>

<ul>
  <li>The tree of the rules <em>not</em> starting with the current entry
    name nor by "<tt>SELF</tt>".</li>
  <li>The tree of the rules starting with the current entry name or by
    the identifier "<tt>SELF</tt>", this symbol not being included in
    the tree.</li>
</ul>

<p>They determine two functions:</p>

<ul>
  <li>The function named "start", analyzing the first tree.</li>
  <li>The function named "continue", taking, as parameter, a value
    previously parsed, and analyzing the second tree.</li>
</ul>

<p>A call to an entry, using "<tt>Grammar.Entry.parse</tt>" correspond
  to a call to the "start" function of the first level of the
  entry.</p>

<p>The "start" function tries its associated tree. If it works, it
  calls the "continue" function of the same level, giving the result
  of "start" as parameter. If this "continue" function fails, this
  parameter is simply returned. If the "start" function fails, the
  "start" function of the next level is tested. If there is no more
  levels, the parsing fails.</p>

<p>The "continue" function first tries the "continue" function of the
  next level. If it fails, or if it is the last level, it tries its
  associated tree, then calls itself again, giving the result as
  parameter. If its associated tree fails, it returns its extra
  parameter.</p>

<h3>Associativity</h3>

<p>While testing the tree, there is a special case for rules ending
  with SELF or with the current entry name. For this last symbol,
  there is a call to the "start" function: of the current level if the
  level is right associative, or of the next level otherwise.</p>

<p>There is no behaviour difference between left and non associative,
  because, in case of syntax error, the system attempts, anyway, to
  recover the error by applying the "continue" function of the
  previous symbol (if this symbol is a call to an entry).</p>

<p>When a SELF or the current entry name is encountered in the middle
  of the rule (i.e. if it is not the last symbol), there is a call to
  the "start" function of the first level of the current entry.</p>

<p>Example. Let us consider the following grammar:</p>

<pre>
  EXTEND
    expr:
      [ "minus" LEFTA
        [ x = SELF; "-"; y = SELF -> x -. y ]
      | "power" RIGHTA
        [ x = SELF; "**"; y = SELF -> x ** y ]
      | "simple"
        [ "("; x = SELF; ")" -> x
        | x = INT -> float_of_int x ] ]
    ;
  END
</pre>

<p>The left "SELF"s of the two levels "minus" and "power" correspond
  to a call to the next level. In the level "minus", the right "SELF"
  also, and the left associativity is treated by the fact that the
  "continue" function is called (starting with the keyword "-" since
  the left "SELF" is not part of the tree). On the other hand, for the
  level "power", the right "SELF" corresponds to a call to the current
  level, i.e. the level "power" again. At end, the "SELF" between
  parentheses of the level "simple" correspond to a call to the first
  level, namely "minus" in this grammar.</p>

<h3>Errors and recovery</h3>

<p>Like for stream parsers, two exceptions may happen:
  "Stream.Failure" or "Stream.Error". The first one indicates that the
  parsing just could not start. The second one indicates that the
  parsing started but failed further.</p>

<p>In stream parsers, when the first symbol of a rule has been
  accepted, all the symbols of the same rule must be accepted,
  otherwise the exception "Stream.Error" is raised.</p>

<p>Here, in extensible grammars, unlike stream parsers, before the
  "Stream.Error" exception, the system attempts to recover the error
  by the following trick: if the previous symbol of the rule was a
  call to another entry, the system calls the "continue" function of
  that entry, which may resolve the problem.</p>

<p>In extensible grammars, the exceptions are encapsulated with the
  exception "Stdpp.Exc_located" giving the location of the error
  together with the exception itself.</p>

<h3>Tokens starting rules</h3>

<p>Another improvement (than the error recovery) is the fact that,
  when a rule starts with several tokens and/or keywords, all these
  tokens and keywords are tested in one time, and the possible
  "Stream.Error" may happen, only from the symbol following them on,
  if any.</p>

<h3>Kind of grammar</h3>

<p>The kind of grammar is predictive parsing grammar, i.e. recursive
  descent parsing without backtrack. But with some nuances, due to the
  improvements (error recovery and token starting rules) indicated in
  the previous sections.</p>

<h2>The Grammar module</h2>

<p>The Grammar module contains all what is necessary to manipulate
  grammars and entries. It contains:</p>

<h3>Main types and values</h3>

<dl>
  <dt><tt>type g = 'abstract;</tt></dt>
  <dd>
    The type of grammars, holding entries.
  </dd>
</dl>

<dl>
  <dt><tt>value gcreate : Token.glexer (string * string) -> g;</tt></dt>
  <dd>
    Create a new grammar, without keywords, using the lexer given as
    parameter.
  </dd>
</dl>

<dl>
  <dt><tt>value tokens : g -> string -> list (string * int);</tt></dt>
  <dd>
    Given a grammar and a token pattern constructor, returns the list of
    the corresponding values currently used in all entries of this grammar.
    The integer is the number of times this pattern value is used.

    Examples:
    <ul>
      <li>The call [<tt>Grammar.tokens g ""</tt>] returns the keywords
        list.</li>
      <li>The call [<tt>Grammar.tokens g "IDENT"</tt>] returns the
        list of all usages of the pattern "IDENT" in
        the <tt>EXTEND</tt> statements.</li>
    </ul>
  </dd>
</dl>

<dl>
  <dt><tt>value glexer : g -> Token.glexer token;</tt></dt>
  <dd>
    Return the lexer used by the grammar
  </dd>
</dl>

<dl>
  <dt><tt>type parsable = 'abstract;</tt></dt>
  <dt><tt>value parsable : g -> Stream.t char -> parsable;</tt></dt>
  <dd>
    Type and value allowing to keep the same token stream between
    several calls of entries of the same grammar, to prevent loss of
    tokens. To be used with <tt>Entry.parse_parsable</tt> below
  </dd>
</dl>

<pre style="border:0; margin-left: 1cm">
module Entry =
  sig
    type e 'a = 'x;
    value create : g -> string -> e 'a;
    value parse : e 'a -> Stream.t char -> 'a;
    value parse_token : e 'a -> Stream.t token -> 'a;
    value parse_parsable : e 'a -> parsable -> 'a;
    value name : e 'a -> string;
    value of_parser : g -> string -> (Stream.t token -> 'a) -> e 'a;
    value print : e 'a -> unit;
    value find : e 'a -> string -> e Obj.t;
    external obj : e 'a -> Gramext.g_entry token = "%identity";
  end;
</pre>

<dl><dd>
    Module to handle entries.
    <ul>
      <li>/Entry.e/ is the type for entries returning values of type
        ['a].</li>
      <li>/Entry.create g n/ creates a new entry named [n] in the
        grammar [g].</li>
      <li>/Entry.parse e/ returns the stream parser of the entry
        [e].</li>
      <li>/Entry.parse_token e/ returns the token parser of the entry
        [e].</li>
      <li>/Entry.parse_parsable e/ returns the parsable parser of the
        entry [e].</li>
      <li>/Entry.name e/ returns the name of the entry [e].</li>
      <li>/Entry.of_parser g n p/ makes an entry from a token stream
        parser.</li>
      <li>/Entry.print e/ displays the entry [e] using [Format].</li>
      <li>/Entry.find e s/ finds the entry named [s] in [e]'s
        rules.</li>
      <li>/Entry.obj e/ converts an entry into a [Gramext.g_entry]
        allowing to see what it holds ([Gramext] is visible, but not
        documented).</li>
    </ul>
</dd></dl>

<dl>
  <dt><tt>value of_entry : Entry.e 'a -> g;</tt></dt>
  <dd>Return the grammar associated with an entry.</dd>
</dl>

<h3>Clearing grammars and entries</h3>

<pre style="border:0; margin-left: 1cm">
module Unsafe :
  sig
    value gram_reinit : g -> Token.glexer token -> unit;
    value clear_entry : Entry.e 'a -> unit;
  end;
</pre>

<dl><dd>
   Module for clearing grammars and entries. To be manipulated
   with care, because: 1) reinitializing a grammar destroys all tokens
   and there may have problems with the associated lexer if there are
   keywords; 2) clearing an entry does not destroy the tokens used
   only by itself.
   <ul>
     <li>/Unsafe.reinit_gram g lex/ removes the tokens of the grammar
       and sets [lex] as a new lexer for [g]. Warning: the lexer
       itself is not reinitialized.</li>
     <li>/Unsafe.clear_entry e/ removes all rules of the entry [e].</li>
   </ul>
</dd></dl>

<h3>Functorial interface</h3>

<p>Alternative for grammars use. Grammars are no more Ocaml values:
  there is no type for them. Modules generated preserve the rule "an
  entry cannot call an entry of another grammar" by normal OCaml
  typing.</p>

<pre style="border:0; margin-left: 1cm">
module type GLexerType =
  sig
    type te = 'x;
    value lexer : Token.glexer te;
  end;
</pre>

<dl><dd>
    The input signature for the functor [Grammar.GMake]: [te] is the
    type of the tokens.
</dd></dl>

<pre style="border:0; margin-left: 1cm">
module type S =
  sig
    type te = 'x;
    type parsable = 'x;
    value parsable : Stream.t char -> parsable;
    value tokens : string -> list (string * int);
    value glexer : Token.glexer te;
    module Entry :
      sig
        type e 'a = 'y;
        value create : string -> e 'a;
        value parse : e 'a -> parsable -> 'a;
        value parse_token : e 'a -> Stream.t te -> 'a;
        value name : e 'a -> string;
        value of_parser : string -> (Stream.t te -> 'a) -> e 'a;
        value print : e 'a -> unit;
        external obj : e 'a -> Gramext.g_entry te = "%identity";
      end;
    module Unsafe :
      sig
        value gram_reinit : Token.glexer te -> unit;
        value clear_entry : Entry.e 'a -> unit;
      end;
    value extend :
      Entry.e 'a -> option Gramext.position ->
        list
          (option string * option Gramext.g_assoc *
           list (list (Gramext.g_symbol te) * Gramext.g_action)) ->
        unit;
    value delete_rule : Entry.e 'a -> list (Gramext.g_symbol te) -> unit;
  end;
</pre>

<dl><dd>
    Signature type of the functor [Grammar.GMake]. The types and
    functions are almost the same than in generic interface, but:
    <ul>
      <li>Grammars are not values. Functions holding a grammar as
         parameter do not have this parameter yet.</li>
      <li>The type [parsable] is used in function [parse] instead of
         the char stream, avoiding the possible loss of tokens.</li>
      <li>The type of tokens (expressions and patterns) can be any
         type (instead of (string * string)); the module parameter
         must specify a way to show them as (string * string).</li>
    </ul>
</dd></dl>

<dl>
  <dt><tt>module GMake (L : GLexerType) : S with type te = L.te;</tt></dt>
  <dd></dd>
</dl>

<h3>Miscellaneous</h3>

<dl>
  <dt><tt>value error_verbose : ref bool;</tt></dt>
  <dd>
    Flag for displaying more information in case of parsing error;
    default = [False].
  </dd>
</dl>

<dl>
  <dt><tt>value warning_verbose : ref bool;</tt></dt>
  <dd>
    Flag for displaying warnings while extension; default = [True].
  </dd>
</dl>

<dl>
  <dt><tt>value strict_parsing : ref bool;</tt></dt>
  <dd>
    Flag to apply strict parsing, without trying to recover errors;
    default = [False].
  </dd>
</dl>

<dl>
  <dt><tt>value print_entry : Format.formatter -> Gramext.g_entry 'te -> unit;</tt></dt>
  <dd>
    General printer for all kinds of entries (obj entries).
  </dd>
</dl>

<dl>
  <dt><tt>value iter_entry : (Gramext.g_entry 'te -> unit) -> Gramext.g_entry 'te -> unit;</tt></dt>
  <dd>
    [Grammar.iter_entry f e] applies [f] to the entry [e] and
    transitively all entries called by [e]. The order in which
    the entries are passed to [f] is the order they appear in
    each entry. Each entry is passed only once. *)
  </dd>
</dl>

<dl>
  <dt><tt>value fold_entry : (Gramext.g_entry 'te -> 'a -> 'a) -> Gramext.g_entry 'te -> 'a -> 'a;</tt></dt>
  <dd>
    [Grammar.fold_entry f e init] computes [(f eN .. (f e2 (f e1 init)))],
    where [e1 .. eN] are [e] and transitively all entries called by [e].
    The order in which the entries are passed to [f] is the order they
    appear in each entry. Each entry is passed only once. *)
  </dd>
</dl>

<h2>Interface with the lexer</h2>

<p>To create a grammar, the function "<tt>Grammar.gcreate</tt>" must
  be called, with a lexer as parameter.</p>

<p>A simple solution, as possible lexer, is the predefined lexer built
  by "<tt>Plexer.gmake ()</tt>", lexer used for the ocaml grammar of
  camlp5. In this case, you can just put it as parameter of
  "<tt>Grammar.gcreate</tt>" and it is not necessary to read this
  section.</p>

<p>The section first introduces the notion of "token patterns" which
  are the way the tokens and keywords symbols in the EXTEND statement
  are represented. Then follow the description of the type of the
  parameter of "<tt>Grammar.gcreate</tt>".</p>

<h3>Token patterns</h3>

<p>A token pattern is a value of the type defined like this:</p>

<pre>
  type pattern = (string * string);
</pre>

<p>This type represents values of the token and keywords symbols in
  the grammar rules.</p>

<p>For a token symbol in the grammar rules, the first string is the
  token constructor name (starting with an uppercase character), the
  second string indicates whether the match is "any" (the empty
  string) or some specific value of the token (an non-empty
  string).</p>

<p>For a keyword symbol, the first string is empty and the second
  string is the keyword itself.</p>

<p>For example, given this grammar rule:</p>

<pre>
  "for"; i = LIDENT; "="; e1 = SELF; "to"; e2 = SELF
</pre>

<p>the different symbols and keywords are represented by the following
  couples of strings:</p>

<ul>
  <li>the keyword "for" is represented by <tt>("", "for")</tt>,</li>
  <li>the keyword "=" by <tt>("", "=")</tt>,</li>
  <li>the keyword "to" by <tt>("", "to")</tt>),</li>
  <li>and the token symbol <tt>LIDENT</tt> by <tt>("LIDENT", "")</tt>.</li>
</ul>

<p>The symbol <tt>UIDENT "Foo"</tt> in a rule would be represented
  by the token pattern:</p>

<pre>
  ("UIDENT", "Foo")
</pre>

<p>Notice that the symbol "<tt>SELF</tt>" is a specific symbol of the
  EXTEND syntax: it does not correspond to a token pattern and is
  represented differently. A token constructor name must not belong to
  the specific symbols: SELF, NEXT, LIST0, LIST1 and OPT.</p>

<h3>The glexer record</h3>

<p>The type of the parameter of the function "<tt>Grammar.gcreate</tt>"
  is "<tt>glexer</tt>", defined in the module "<tt>Token</tt>". It is
  a record type with the following fields:</p>

<h4><tt>tok_func</tt></h4>

<p>It is the lexer itself. Its type is:</p>

<pre>
  Stream.t char -> (Stream.t (string * string) * location_function);
</pre>

<p>The lexer takes a character stream as parameter and must answer a
  couple of: a token stream, the tokens being represented by a couple
  of strings, and a location function.</p>

<p>The location function is a function taking, as parameter, a integer
  corresponding to a token number in the stream (starting from zero),
  and returning the location of this token in the source. It is
  important to get the good locations in the semantic actions of the
  grammar rules.</p>

<p>Notice that, despite the lexer takes a character stream as
  parameter, it is not mandatory to use the stream parsers technology
  to write the lexer. What is important is that it does the job.</p>

<h4><tt>tok_using</tt></h4>

<p>It is a function of type:</p>

<pre>
  pattern -> unit
</pre>

<p>The parameter of this function is the representation of a token
  symbol or a keyword symbol in grammar rules. See the section about
  token patterns.</p>

<p>This function is called for each token symbol and each keyword
  encountered in the grammar rules of the EXTEND statement. Its goal
  is to allow the lexer to check that the tokens and keywords do
  respect the lexer rules. It checks that the tokens exist and are not
  mispelled. It can be also used to enter the keywords in the lexer
  keyword tables.</p>

<p>Setting it as the function that does nothing is possible, but the
  check of correctness of tokens is not done.</p>

<p>In case or error, the function must raise the exception
  "<tt>Token.Error</tt>" with an error message as parameter.</p>

<h4><tt>tok_removing</tt></h4>

<p>It is a function of type:</p>

<pre>
  pattern -> unit
</pre>

<p>It is called by the DELETE_RULE statement for each token symbol and
  each keyword that an occurence of them is no more used. This can be
  interesting for keywords, if the lexer record the number of
  occurences of the keywords: when the number of occurences falls to
  zero, the keyword can be removed from the lexer tables.</p>

<h4><tt>tok_match</tt></h4>

<p>It is a function of type:</p>

<pre>
  pattern -> ((string * string) -> unit)
</pre>

<p>The function tells how a token of the input stream is matched
  against a token pattern. Both are represented by a couple of
  strings.</p>

<p>This function takes a token pattern as parameter and return a
  function matching a token, returning the matched string or raising
  the exception "<tt>Stream.Failure</tt>" if the token does not
  match.</p>

<p>Notice that, for efficiency, it is necessary to write this function
  as a match of token patterns returning, for each case, the function
  which matches the token, <em>not</em> a function matching the token
  pattern and the token together and returning a string for each
  case.</p>

<p>An acceptable function is provided in the module "Token" and is
  named "default_match". Its code looks like this:</p>

<pre>
  value default_match =
    fun
    [ (p_con, "") ->
        fun (con, prm) -> if con = p_con then prm else raise Stream.Failure
    | (p_con, p_prm) ->
        fun (con, prm) ->
          if con = p_con &amp;&amp; prm = p_prm then prm else raise Stream.Failure ]
  ;
</pre>

<h4><tt>tok_text</tt></h4>

<p>It is a function of type:</p>

<pre>
  pattern -> string
</pre>

<p>Destinated to error messages, it takes a token pattern as parameter
  and return the string giving its name.</p>

<p>It is possible to use the predefined function "<tt>lexer_text</tt>"
  of the Token module. This function just returns the name of the
  token pattern constructor and its parameter if any.</p>

<p>For example, with this default function, the token symbol IDENT
  would be written as IDENT in error message (e.g. "IDENT expected").
  The "text" function may decide to print it differently, e.g., as
  "identifier".</p>

<h4><tt>tok_comm</tt></h4>

<p>It is a mutable field of type:</p>

<pre>
  option (list location)
</pre>

<p>It asks the lexer (the lexer function should do it) to record the
  locations of the comments in the program. Setting this field to
  "None" indicates that the lexer must not record them. Setting it to
  "Some []" indicated that the lexer must put the comments location
  list in the field, which is mutable.</p>

<h3>Minimalist version</h3>

<p>If a lexer have been written, named "<tt>lexer</tt>", here is the
  minimalist version of the value suitable as parameter to
  "<tt>Grammar.gcreate</tt>":</p>

<pre>
  {Token.tok_func = lexer;
   Token.tok_using _ = (); Token.tok_removing _ = ();
   Token.tok_match = Token.default_match;
   Token.tok_text = Token.lexer_text;
   Token.tok_comm = None}
</pre>

<h2>Functorial interface</h2>

<p>The normal interface for grammars described in the previous sections
  has two drawbacks:</p>

<ul>
  <li>First, the type of tokens of the lexers must be "<tt>(string *
      string)</tt>"</li>
  <li>Second, since the entry type has no parameter to specify the
    grammar it is bound to, there is no static check that entries are
    compatible, i.e.  belong to the same grammar. The check is done at
    run time.</li>
</ul>

<p>The functorial interface resolve these two problems. The functor
  takes a module as parameter where the token type has to be defined,
  together with the lexer returning streams of tokens of this
  type. The resulting module define entries compatible the ones to the
  other, and this is controlled by the ocaml type checker.</p>

<p>The syntax extension must be done with the statement GEXTEND, instead
  of EXTEND, and deletion by GDELETE_RULE instead of DELETE_RULE.</p>

<h3>The glexer type</h3>

<p>In the section about the interface with the lexer, we presented the
  glexer type as a record without type parameter. Actually, this type
  is defined as:</p>

<pre>
  type glexer 'te =
    { tok_func : lexer_func 'te;
      tok_using : pattern -> unit;
      tok_removing : pattern -> unit;
      tok_match : pattern -> 'te -> string;
      tok_text : pattern -> string;
      tok_comm : mutable option (list location) }
  ;
</pre>

<p>where the type parameter is the type of the token, which can be any
  type, different from "<tt>(string * string)</tt>", providing the
  lexer function (<tt>tok_func</tt>) returns a stream of this token
  type and the match function (<tt>tok_match</tt>) indicates how to
  match values of this token type against the token patterns (which
  remain defined as "<tt>(string * string)</tt>").</p>

<p>Here is an example of an user token type and the associated match
  function:</p>

<pre>
  type mytoken = [ Ident of string | Int of int | Comma | Equal | Keyw of string  ];

  value mymatch =
    fun
    [ ("IDENT", "") -> fun [ Ident s -> s | _ -> raise Stream.Failure ]
    | ("INT", "") -> fun [ Int i -> string_of_int i | _ -> raise Stream.Failure ]
    | ("", ",") -> fun [ Comma -> "" | _ -> raise Stream.Failure ]
    | ("", "=") -> fun [ Equal -> "" | _ -> raise Stream.Failure ]
    | ("", s) ->
        fun
        [ Keyw k -> if k = s then "" else raise Stream.Failure
        | _ -> raise Stream.Failure ]
    | _ -> raise (Token.Error "bad token in match function") ]
  ;
</pre>

<h3>The functor parameter</h3>

<p>The type of the functor parameter is defined as:</p>

<pre>
  module type GLexerType =
    sig
      type te = 'x;
      value lexer : Token.glexer te;
    end;
</pre>

<p>The token type must be specified (type "<tt>te</tt>") and the lexer
  also, with the interface for lexers, of the glexer type defined
  above, the record fields being described in the section "interface
  with the lexer", but with a general token type.</p>

<h3>The resulting grammar module</h3>

<p>Once a module of type "<tt>GLexerType</tt>" has been built (previous
  section, it is possible to create a grammar module by applying the
  functor "<tt>Grammar.GMake</tt>". For example:</p>

<pre>
  module MyGram = Grammar.GMake MyLexer;
</pre>

<p>Notice that the function "<tt>Entry.parse</tt>" of this resulting
  module does not take a character stream as parameter, but a value of
  type "<tt>parsable</tt>". This function is equivalent to the
  function "<tt>parse_parsable</tt>" of the non functorial
  interface. In short, the parsing of some character stream
  "<tt>cs</tt>" by some entry "<tt>e</tt>" of the example grammar
  above, must be done by:</p>

<pre>
  MyGram.Entry.parse e (MyGram.parsable cs)
</pre>

<p>instead of:</p>

<pre>
  MyGram.Entry.parse e cs
</pre>

<h3>GEXTEND and GDELETE_RULE</h3>

<p>The "<tt>GEXTEND</tt>" and "<tt>GDELETE_RULE</tt>" statements are
  also added in the expressions of the ocaml language when the syntax
  extension kit "<tt>pa_extend.cmo</tt>" is loaded. They have to be
  used for grammars defined with the functorial interface. Their
  syntax are:</p>

<pre>
           expression ::= gextend
                        | gdelete-rule
         gdelete-rule ::= "GDELETE_RULE" gdelete-rule-body "END"
              gextend ::= "GEXTEND" gextend-body "END"
         gextend-body ::= grammar-module-name extend-body
    gdelete-rule-body ::= grammar-module-name delete-rule-body
  grammar-module-name ::= qualid
</pre>

<p>See the syntax of the EXTEND statement for the meaning of the syntax
  entries not defined above.</p>

<div class="trailer">
</div>

</div>

</body>
</html>
